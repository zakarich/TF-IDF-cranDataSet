{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS = ['the green car is fast',\n",
    " 'The blue car has low mileage',\n",
    " 'Your car has high mileage',\n",
    " 'I buy green shirt because it fits my blue eyes']\n",
    "new_doc = ['I want to buy a car']\n",
    "type(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
      "  0 0]\n",
      " [0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0\n",
      "  0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  1 1]\n",
      " [1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
      "  0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def bow_generator(corpus, ngram_range=(1,2)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "# build bow vectorizer and get features\n",
    "bow_vectorizer, bow_features = bow_generator(CORPUS)\n",
    "features = bow_features.todense()\n",
    "print (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['because', 'because it', 'blue', 'blue car', 'blue eyes', 'buy', 'buy green', 'car', 'car has', 'car is', 'eyes', 'fast', 'fits', 'fits my', 'green', 'green car', 'green shirt', 'has', 'has high', 'has low', 'high', 'high mileage', 'is', 'is fast', 'it', 'it fits', 'low', 'low mileage', 'mileage', 'my', 'my blue', 'shirt', 'shirt because', 'the', 'the blue', 'the green', 'your', 'your car']\n"
     ]
    }
   ],
   "source": [
    "# print the feature names\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print (feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0]]\n"
     ]
    }
   ],
   "source": [
    "# extract features from new document using built vectorizer\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print(new_doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>because</th>\n",
       "      <th>because it</th>\n",
       "      <th>blue</th>\n",
       "      <th>blue car</th>\n",
       "      <th>blue eyes</th>\n",
       "      <th>buy</th>\n",
       "      <th>buy green</th>\n",
       "      <th>car</th>\n",
       "      <th>car has</th>\n",
       "      <th>car is</th>\n",
       "      <th>...</th>\n",
       "      <th>mileage</th>\n",
       "      <th>my</th>\n",
       "      <th>my blue</th>\n",
       "      <th>shirt</th>\n",
       "      <th>shirt because</th>\n",
       "      <th>the</th>\n",
       "      <th>the blue</th>\n",
       "      <th>the green</th>\n",
       "      <th>your</th>\n",
       "      <th>your car</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   because  because it  blue  blue car  blue eyes   buy  buy green   car  \\\n",
       "0     0.00        0.00  0.00      0.00       0.00  0.00       0.00  0.23   \n",
       "1     0.00        0.00  0.27      0.34       0.00  0.00       0.00  0.22   \n",
       "2     0.00        0.00  0.00      0.00       0.00  0.00       0.00  0.24   \n",
       "3     0.25        0.25  0.20      0.00       0.25  0.25       0.25  0.00   \n",
       "\n",
       "   car has  car is  ...  mileage    my  my blue  shirt  shirt because   the  \\\n",
       "0     0.00    0.36  ...     0.00  0.00     0.00   0.00           0.00  0.29   \n",
       "1     0.27    0.00  ...     0.27  0.00     0.00   0.00           0.00  0.27   \n",
       "2     0.29    0.00  ...     0.29  0.00     0.00   0.00           0.00  0.00   \n",
       "3     0.00    0.00  ...     0.00  0.25     0.25   0.25           0.25  0.00   \n",
       "\n",
       "   the blue  the green  your  your car  \n",
       "0      0.00       0.36  0.00      0.00  \n",
       "1      0.34       0.00  0.00      0.00  \n",
       "2      0.00       0.00  0.37      0.37  \n",
       "3      0.00       0.00  0.00      0.00  \n",
       "\n",
       "[4 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(norm='l2',smooth_idf=True, use_idf=True) #smoothing gives sme weights to terms with zero idf not to ignore them.\n",
    "tfidf_matrix = transformer.fit_transform(bow_features)\n",
    "features = np.round(tfidf_matrix.todense(), 2)\n",
    " \n",
    "df_tfidf = pd.DataFrame(data=features, columns=feature_names)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>because</th>\n",
       "      <th>because it</th>\n",
       "      <th>blue</th>\n",
       "      <th>blue car</th>\n",
       "      <th>blue eyes</th>\n",
       "      <th>buy</th>\n",
       "      <th>buy green</th>\n",
       "      <th>car</th>\n",
       "      <th>car has</th>\n",
       "      <th>car is</th>\n",
       "      <th>...</th>\n",
       "      <th>mileage</th>\n",
       "      <th>my</th>\n",
       "      <th>my blue</th>\n",
       "      <th>shirt</th>\n",
       "      <th>shirt because</th>\n",
       "      <th>the</th>\n",
       "      <th>the blue</th>\n",
       "      <th>the green</th>\n",
       "      <th>your</th>\n",
       "      <th>your car</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   because  because it  blue  blue car  blue eyes   buy  buy green   car  \\\n",
       "0      0.0         0.0   0.0       0.0        0.0  0.71        0.0  0.71   \n",
       "\n",
       "   car has  car is  ...  mileage   my  my blue  shirt  shirt because  the  \\\n",
       "0      0.0     0.0  ...      0.0  0.0      0.0    0.0            0.0  0.0   \n",
       "\n",
       "   the blue  the green  your  your car  \n",
       "0       0.0        0.0   0.0       0.0  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf features for new_doc\n",
    "tfidf_matrix = transformer.fit_transform(new_doc_features)\n",
    "features_new = np.round(tfidf_matrix.todense(), 2)\n",
    " \n",
    "df_tfidf_new = pd.DataFrame(data=features_new, columns=feature_names)\n",
    "df_tfidf_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07155872  0.05740496 -0.02537698 -0.026161   -0.074492  ]\n",
      "[ 0.09699327  0.02634738  0.01832688  0.06455417 -0.08762705]\n"
     ]
    }
   ],
   "source": [
    "import gensim # commonly used for semantic analysis, topic modeling and similarity analysis.\n",
    "import nltk\n",
    "# tokenize corpora\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) for sentence in new_doc]\n",
    "# build the word2vec model on our training corpus\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, size=5, window=2, min_count=1)\n",
    "print (model.wv['green'])\n",
    "print (model.wv['blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-70b1fb9131af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'woman'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'king'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'man'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m [('queen', 0.7118192911148071),\n\u001b[0;32m      5\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[1;34m'monarch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6189674139022827\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    384\u001b[0m                     \u001b[1;31m# TODO use frombuffer or something similar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                 \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36madd_word\u001b[1;34m(word, weights)\u001b[0m\n\u001b[0;32m    364\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vocabulary file is incomplete: '%s' is missing\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\n",
    "[('queen', 0.7118192911148071),\n",
    " ('monarch', 0.6189674139022827),\n",
    " ('princess', 0.5902431607246399)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'green', 'car', 'is', 'fast'], ['The', 'blue', 'car', 'has', 'low', 'mileage'], ['Your', 'car', 'has', 'high', 'mileage'], ['I', 'buy', 'green', 'shirt', 'because', 'it', 'fits', 'my', 'blue', 'eyes']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.most_similar(positive=['robots', 'future', 'humans'], topn=2)\n",
    "[('autonomous_robots', 0.6404092907905579),\n",
    " ('humanoid_robots', 0.6080278754234314)]\n",
    " \n",
    "print(\"The similarity between 'United_States' \" + \"and 'Canada':\", model.similarity('United_States', 'Canada')) \n",
    "print(\"The similarity between 'United_States' \" + \"and 'Tunisia':\", model.similarity('United_States', 'Tunisia')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term that does not fit in\n",
    "model.doesnt_match(\"bank debt credit banana\".split())\n",
    "'banana'\n",
    " \n",
    "model.distance(\"man\",\"woman\") # distance = 1- similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "sentences = brown.sents()\n",
    "sentences\n",
    " \n",
    "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n",
    " \n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n",
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\n",
    " \n",
    "[('Eastwick', 0.9702203273773193),\n",
    " ('80', 0.9639546871185303),\n",
    " ('diameter', 0.9638100266456604)]\n",
    " \n",
    "model.wv.most_similar(positive=['robots', 'future', 'humans'], topn=2)\n",
    "[('compromise', 0.9315764904022217), ('benefits', 0.9312320947647095)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "common_texts[0:3]\n",
    " \n",
    "[['human', 'interface', 'computer'],\n",
    " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    " ['eps', 'user', 'interface', 'system']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(common_texts)# we assigned a unique integer id to all words appearing in the corpus with the gensim Dictionary class.\n",
    "print(dictionary) #each document will be represented by the number of distinct words in the corpus (total vector size))\n",
    "\n",
    "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n",
    "\n",
    "print(dictionary.token2id) # he mapping between words and their ids\n",
    "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in common_texts] # each word has an Id and each document has words\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "documents\n",
    " \n",
    "[TaggedDocument(words=['human', 'interface', 'computer'], tags=[0]),\n",
    " TaggedDocument(words=['survey', 'user', 'computer', 'system', 'response', 'time'], tags=[1]),\n",
    " TaggedDocument(words=['eps', 'user', 'interface', 'system'], tags=[2]),\n",
    " TaggedDocument(words=['system', 'human', 'system', 'eps'], tags=[3]),\n",
    " TaggedDocument(words=['user', 'response', 'time'], tags=[4]),\n",
    " TaggedDocument(words=['trees'], tags=[5]),\n",
    " TaggedDocument(words=['graph', 'trees'], tags=[6]),\n",
    " TaggedDocument(words=['graph', 'minors', 'trees'], tags=[7]),\n",
    " TaggedDocument(words=['graph', 'minors', 'survey'], tags=[8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  building a model\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, dm =1)\n",
    "#Infer vector for a new document:\n",
    "vector = model.infer_vector([\"system\", \"response\"])\n",
    "vector\n",
    "array([-0.03828789,  0.06952108,  0.02473402,  0.01858516, -0.08879752],\n",
    "      dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
